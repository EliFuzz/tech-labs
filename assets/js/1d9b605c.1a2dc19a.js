"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[6051],{98589:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>y,contentTitle:()=>f,default:()=>w,frontMatter:()=>x,metadata:()=>i,toc:()=>T});const i=JSON.parse('{"id":"education/computer-science/algorithms/algo/nano-neuron","title":"NanoNeuron","description":"NanoNeuron","source":"@site/docs/education/01-computer-science/10-algorithms/04-algo/nano-neuron.mdx","sourceDirName":"education/01-computer-science/10-algorithms/04-algo","slug":"/education/computer-science/algorithms/algo/nano-neuron","permalink":"/tech-labs/docs/education/computer-science/algorithms/algo/nano-neuron","draft":false,"unlisted":false,"editUrl":"https://github.com/EliFuzz/tech-labs/docs/education/01-computer-science/10-algorithms/04-algo/nano-neuron.mdx","tags":[],"version":"current","frontMatter":{"title":"NanoNeuron","description":"NanoNeuron","hide_table_of_contents":true},"sidebar":"education","previous":{"title":"N-Queens Problem","permalink":"/tech-labs/docs/education/computer-science/algorithms/algo/n-queens-problem"},"next":{"title":"Palindrome","permalink":"/tech-labs/docs/education/computer-science/algorithms/algo/palindrome"}}');var r=t(23420),a=t(38906),o=t(31519),s=t(20007),d=t(20636);const l='package main\n\nimport (\n\t"math"\n)\n\ntype NanoNeuron struct {\n\tw float64\n\tb float64\n}\n\nfunc NewNanoNeuron(w, b float64) *NanoNeuron {\n\treturn &NanoNeuron{w: w, b: b}\n}\n\nfunc (nn *NanoNeuron) Predict(x float64) float64 {\n\treturn x*nn.w + nn.b\n}\n\nfunc CelsiusToFahrenheit(c float64) float64 {\n\tw := 1.8\n\tb := 32.0\n\tf := c*w + b\n\treturn f\n}\n\nfunc GenerateDataSets() ([]float64, []float64, []float64, []float64) {\n\txTrain := make([]float64, 0)\n\tyTrain := make([]float64, 0)\n\n\tfor x := 0.0; x < 100; x += 1 {\n\t\ty := CelsiusToFahrenheit(x)\n\t\txTrain = append(xTrain, x)\n\t\tyTrain = append(yTrain, y)\n\t}\n\n\txTest := make([]float64, 0)\n\tyTest := make([]float64, 0)\n\n\tfor x := 0.5; x < 100; x += 1 {\n\t\ty := CelsiusToFahrenheit(x)\n\t\txTest = append(xTest, x)\n\t\tyTest = append(yTest, y)\n\t}\n\n\treturn xTrain, yTrain, xTest, yTest\n}\n\nfunc PredictionCost(y, prediction float64) float64 {\n\treturn math.Pow(y-prediction, 2) / 2\n}\n\nfunc ForwardPropagation(model *NanoNeuron, xTrain, yTrain []float64) ([]float64, float64) {\n\tm := len(xTrain)\n\tpredictions := make([]float64, 0)\n\tcost := 0.0\n\n\tfor i := 0; i < m; i++ {\n\t\tprediction := model.Predict(xTrain[i])\n\t\tcost += PredictionCost(yTrain[i], prediction)\n\t\tpredictions = append(predictions, prediction)\n\t}\n\n\tcost /= float64(m)\n\treturn predictions, cost\n}\n\nfunc BackwardPropagation(predictions, xTrain, yTrain []float64) (float64, float64) {\n\tm := len(xTrain)\n\tdW := 0.0\n\tdB := 0.0\n\n\tfor i := 0; i < m; i++ {\n\t\tdW += (yTrain[i] - predictions[i]) * xTrain[i]\n\t\tdB += yTrain[i] - predictions[i]\n\t}\n\n\tdW /= float64(m)\n\tdB /= float64(m)\n\n\treturn dW, dB\n}\n\nfunc TrainModel(model *NanoNeuron, epochs int, alpha float64, xTrain, yTrain []float64) []float64 {\n\tcostHistory := make([]float64, 0)\n\n\tfor epoch := 0; epoch < epochs; epoch++ {\n\t\tpredictions, cost := ForwardPropagation(model, xTrain, yTrain)\n\t\tcostHistory = append(costHistory, cost)\n\n\t\tdW, dB := BackwardPropagation(predictions, xTrain, yTrain)\n\n\t\tmodel.w += alpha * dW\n\t\tmodel.b += alpha * dB\n\t}\n\n\treturn costHistory\n}\n',c="import java.util.ArrayList;\nimport java.util.List;\n\nclass NanoNeuron {\n\n  double w;\n  double b;\n\n  NanoNeuron(double w, double b) {\n    this.w = w;\n    this.b = b;\n  }\n\n  double predict(double x) {\n    return x * this.w + this.b;\n  }\n}\n\npublic class Main {\n\n  public static double celsiusToFahrenheit(double c) {\n    final double w = 1.8;\n    final double b = 32;\n    return c * w + b;\n  }\n\n  public static List<List<Double>> generateDataSets() {\n    List<Double> xTrain = new ArrayList<>();\n    List<Double> yTrain = new ArrayList<>();\n    for (double x = 0; x < 100; x += 1) {\n      double y = celsiusToFahrenheit(x);\n      xTrain.add(x);\n      yTrain.add(y);\n    }\n\n    List<Double> xTest = new ArrayList<>();\n    List<Double> yTest = new ArrayList<>();\n    for (double x = 0.5; x < 100; x += 1) {\n      double y = celsiusToFahrenheit(x);\n      xTest.add(x);\n      yTest.add(y);\n    }\n\n    List<List<Double>> dataSets = new ArrayList<>();\n    dataSets.add(xTrain);\n    dataSets.add(yTrain);\n    dataSets.add(xTest);\n    dataSets.add(yTest);\n    return dataSets;\n  }\n\n  public static double predictionCost(double y, double prediction) {\n    return Math.pow(y - prediction, 2) / 2;\n  }\n\n  public static double[] forwardPropagation(NanoNeuron model, List<Double> xTrain, List<Double> yTrain) {\n    int m = xTrain.size();\n    List<Double> predictions = new ArrayList<>();\n    double cost = 0;\n    for (int i = 0; i < m; i++) {\n      double prediction = model.predict(xTrain.get(i));\n      cost += predictionCost(yTrain.get(i), prediction);\n      predictions.add(prediction);\n    }\n    cost /= m;\n    return new double[]{cost, predictions};\n  }\n\n  public static double[] backwardPropagation(List<Double> predictions, List<Double> xTrain, List<Double> yTrain) {\n    int m = xTrain.size();\n    double dW = 0;\n    double dB = 0;\n    for (int i = 0; i < m; i++) {\n      dW += (yTrain.get(i) - predictions.get(i)) * xTrain.get(i);\n      dB += yTrain.get(i) - predictions.get(i);\n    }\n    dW /= m;\n    dB /= m;\n    return new double[]{dW, dB};\n  }\n\n  public static List<Double> trainModel(NanoNeuron model, int epochs, double alpha, List<Double> xTrain, List<Double> yTrain) {\n    List<Double> costHistory = new ArrayList<>();\n    for (int epoch = 0; epoch < epochs; epoch++) {\n      double[] forwardResult = forwardPropagation(model, xTrain, yTrain);\n      double cost = forwardResult[0];\n      List<Double> predictions = forwardResult[1];\n\n      double[] backwardResult = backwardPropagation(predictions, xTrain, yTrain);\n      double dW = backwardResult[0];\n      double dB = backwardResult[1];\n\n      model.w += alpha * dW;\n      model.b += alpha * dB;\n\n      costHistory.add(cost);\n    }\n    return costHistory;\n  }\n}\n",u="function NanoNeuron(w, b) {\n  this.w = w;\n  this.b = b;\n  this.predict = (x) => {\n    return x * this.w + this.b;\n  };\n}\n\nfunction celsiusToFahrenheit(c) {\n  const w = 1.8;\n  const b = 32;\n  const f = c * w + b;\n  return f;\n}\n\nfunction generateDataSets() {\n  const xTrain = [];\n  const yTrain = [];\n  for (let x = 0; x < 100; x += 1) {\n    const y = celsiusToFahrenheit(x);\n    xTrain.push(x);\n    yTrain.push(y);\n  }\n\n  const xTest = [];\n  const yTest = [];\n  for (let x = 0.5; x < 100; x += 1) {\n    const y = celsiusToFahrenheit(x);\n    xTest.push(x);\n    yTest.push(y);\n  }\n\n  return [xTrain, yTrain, xTest, yTest];\n}\n\nfunction predictionCost(y, prediction) {\n  return (y - prediction) ** 2 / 2;\n}\n\nfunction forwardPropagation(model, xTrain, yTrain) {\n  const m = xTrain.length;\n  const predictions = [];\n  let cost = 0;\n  for (let i = 0; i < m; i += 1) {\n    const prediction = model.predict(xTrain[i]);\n    cost += predictionCost(yTrain[i], prediction);\n    predictions.push(prediction);\n  }\n  cost /= m;\n  return [predictions, cost];\n}\n\nfunction backwardPropagation(predictions, xTrain, yTrain) {\n  const m = xTrain.length;\n  let dW = 0;\n  let dB = 0;\n  for (let i = 0; i < m; i += 1) {\n    dW += (yTrain[i] - predictions[i]) * xTrain[i];\n    dB += yTrain[i] - predictions[i];\n  }\n  dW /= m;\n  dB /= m;\n  return [dW, dB];\n}\n\nfunction trainModel({ model, epochs, alpha, xTrain, yTrain }) {\n  const costHistory = [];\n  for (let epoch = 0; epoch < epochs; epoch += 1) {\n    const [predictions, cost] = forwardPropagation(model, xTrain, yTrain);\n    costHistory.push(cost);\n\n    const [dW, dB] = backwardPropagation(predictions, xTrain, yTrain);\n\n    model.w += alpha * dW;\n    model.b += alpha * dB;\n  }\n  return costHistory;\n}\n",p="class NanoNeuron(val w: Double, val b: Double) {\n    fun predict(x: Double): Double {\n        return x * w + b\n    }\n}\n\nfun celsiusToFahrenheit(c: Double): Double {\n    val w = 1.8\n    val b = 32\n    return c * w + b\n}\n\nfun generateDataSets(): List<List<Double>> {\n    val xTrain = mutableListOf<Double>()\n    val yTrain = mutableListOf<Double>()\n    for (x in 0 until 100) {\n        val y = celsiusToFahrenheit(x.toDouble())\n        xTrain.add(x.toDouble())\n        yTrain.add(y)\n    }\n\n    val xTest = mutableListOf<Double>()\n    val yTest = mutableListOf<Double>()\n    for (x in 0.5 until 100) {\n        val y = celsiusToFahrenheit(x)\n        xTest.add(x)\n        yTest.add(y)\n    }\n\n    return listOf(xTrain, yTrain, xTest, yTest)\n}\n\nfun predictionCost(y: Double, prediction: Double): Double {\n    return Math.pow(y - prediction, 2.0) / 2\n}\n\nfun forwardPropagation(model: NanoNeuron, xTrain: List<Double>, yTrain: List<Double>): Pair<List<Double>, Double> {\n    val m = xTrain.size\n    val predictions = mutableListOf<Double>()\n    var cost = 0.0\n    for (i in 0 until m) {\n        val prediction = model.predict(xTrain[i])\n        cost += predictionCost(yTrain[i], prediction)\n        predictions.add(prediction)\n    }\n    cost /= m\n    return Pair(predictions, cost)\n}\n\nfun backwardPropagation(predictions: List<Double>, xTrain: List<Double>, yTrain: List<Double>): Pair<Double, Double> {\n    val m = xTrain.size\n    var dW = 0.0\n    var dB = 0.0\n    for (i in 0 until m) {\n        dW += (yTrain[i] - predictions[i]) * xTrain[i]\n        dB += yTrain[i] - predictions[i]\n    }\n    dW /= m\n    dB /= m\n    return Pair(dW, dB)\n}\n\nfun trainModel(model: NanoNeuron, epochs: Int, alpha: Double, xTrain: List<Double>, yTrain: List<Double>): List<Double> {\n    val costHistory = mutableListOf<Double>()\n    for (epoch in 0 until epochs) {\n        val (predictions, cost) = forwardPropagation(model, xTrain, yTrain)\n        costHistory.add(cost)\n\n        val (dW, dB) = backwardPropagation(predictions, xTrain, yTrain)\n\n        model.w += alpha * dW\n        model.b += alpha * dB\n    }\n    return costHistory\n}\n",h="class NanoNeuron:\n    def __init__(self, w, b):\n        self.w = w\n        self.b = b\n\n    def predict(self, x):\n        return x * self.w + self.b\n\ndef celsius_to_fahrenheit(c):\n    w = 1.8\n    b = 32\n    return c * w + b\n\ndef generate_data_sets():\n    x_train = []\n    y_train = []\n    for x in range(100):\n        y = celsius_to_fahrenheit(x)\n        x_train.append(x)\n        y_train.append(y)\n\n    x_test = []\n    y_test = []\n    for x in range(1, 100):\n        y = celsius_to_fahrenheit(x)\n        x_test.append(x)\n        y_test.append(y)\n\n    return x_train, y_train, x_test, y_test\n\ndef prediction_cost(y, prediction):\n    return (y - prediction) ** 2 / 2\n\ndef forward_propagation(model, x_train, y_train):\n    m = len(x_train)\n    predictions = []\n    cost = 0\n    for i in range(m):\n        prediction = model.predict(x_train[i])\n        cost += prediction_cost(y_train[i], prediction)\n        predictions.append(prediction)\n    cost /= m\n    return predictions, cost\n\ndef backward_propagation(predictions, x_train, y_train):\n    m = len(x_train)\n    dW = 0\n    dB = 0\n    for i in range(m):\n        dW += (y_train[i] - predictions[i]) * x_train[i]\n        dB += y_train[i] - predictions[i]\n    dW /= m\n    dB /= m\n    return dW, dB\n\ndef train_model(model, epochs, alpha, x_train, y_train):\n    cost_history = []\n    for epoch in range(epochs):\n        predictions, cost = forward_propagation(model, x_train, y_train)\n        cost_history.append(cost)\n\n        dW, dB = backward_propagation(predictions, x_train, y_train)\n\n        model.w += alpha * dW\n        model.b += alpha * dB\n    return cost_history\n",b="struct NanoNeuron {\n    w: f64,\n    b: f64,\n}\n\nimpl NanoNeuron {\n    fn new(w: f64, b: f64) -> Self {\n        NanoNeuron { w, b }\n    }\n\n    fn predict(&self, x: f64) -> f64 {\n        x * self.w + self.b\n    }\n}\n\nfn celsius_to_fahrenheit(c: f64) -> f64 {\n    let w = 1.8;\n    let b = 32.0;\n    c * w + b\n}\n\nfn generate_data_sets() -> (Vec<f64>, Vec<f64>, Vec<f64>, Vec<f64>) {\n    let mut x_train = Vec::new();\n    let mut y_train = Vec::new();\n    for x in 0..100 {\n        let y = celsius_to_fahrenheit(x as f64);\n        x_train.push(x as f64);\n        y_train.push(y);\n    }\n\n    let mut x_test = Vec::new();\n    let mut y_test = Vec::new();\n    for x in (0..100).map(|x| x as f64 + 0.5) {\n        let y = celsius_to_fahrenheit(x);\n        x_test.push(x);\n        y_test.push(y);\n    }\n\n    (x_train, y_train, x_test, y_test)\n}\n\nfn prediction_cost(y: f64, prediction: f64) -> f64 {\n    (y - prediction).powi(2) / 2.0\n}\n\nfn forward_propagation(model: &NanoNeuron, x_train: &[f64], y_train: &[f64]) -> (Vec<f64>, f64) {\n    let m = x_train.len();\n    let mut predictions = Vec::new();\n    let mut cost = 0.0;\n    for i in 0..m {\n        let prediction = model.predict(x_train[i]);\n        cost += prediction_cost(y_train[i], prediction);\n        predictions.push(prediction);\n    }\n    cost /= m as f64;\n    (predictions, cost)\n}\n\nfn backward_propagation(predictions: &[f64], x_train: &[f64], y_train: &[f64]) -> (f64, f64) {\n    let m = x_train.len() as f64;\n    let (mut dW, mut dB) = (0.0, 0.0);\n    for i in 0..x_train.len() {\n        dW += (y_train[i] - predictions[i]) * x_train[i];\n        dB += y_train[i] - predictions[i];\n    }\n    dW /= m;\n    dB /= m;\n    (dW, dB)\n}\n\nfn train_model(model: &mut NanoNeuron, epochs: usize, alpha: f64, x_train: &[f64], y_train: &[f64]) -> Vec<f64> {\n    let mut cost_history = Vec::new();\n    for _ in 0..epochs {\n        let (predictions, cost) = forward_propagation(model, x_train, y_train);\n        cost_history.push(cost);\n\n        let (dW, dB) = backward_propagation(&predictions, x_train, y_train);\n\n        model.w += alpha * dW;\n        model.b += alpha * dB;\n    }\n    cost_history\n}\n",m="class NanoNeuron {\n  w: number;\n  b: number;\n\n  constructor(w: number, b: number) {\n    this.w = w;\n    this.b = b;\n  }\n\n  predict(x: number): number {\n    return x * this.w + this.b;\n  }\n}\n\nfunction celsiusToFahrenheit(c: number): number {\n  const w: number = 1.8;\n  const b: number = 32;\n  const f: number = c * w + b;\n  return f;\n}\n\nfunction generateDataSets(): [number[], number[], number[], number[]] {\n  const xTrain: number[] = [];\n  const yTrain: number[] = [];\n  for (let x = 0; x < 100; x += 1) {\n    const y: number = celsiusToFahrenheit(x);\n    xTrain.push(x);\n    yTrain.push(y);\n  }\n\n  const xTest: number[] = [];\n  const yTest: number[] = [];\n  for (let x = 0.5; x < 100; x += 1) {\n    const y: number = celsiusToFahrenheit(x);\n    xTest.push(x);\n    yTest.push(y);\n  }\n\n  return [xTrain, yTrain, xTest, yTest];\n}\n\nfunction predictionCost(y: number, prediction: number): number {\n  return Math.pow(y - prediction, 2) / 2;\n}\n\nfunction forwardPropagation(\n  model: NanoNeuron,\n  xTrain: number[],\n  yTrain: number[],\n): [number[], number] {\n  const m: number = xTrain.length;\n  const predictions: number[] = [];\n  let cost: number = 0;\n  for (let i = 0; i < m; i += 1) {\n    const prediction: number = model.predict(xTrain[i]);\n    cost += predictionCost(yTrain[i], prediction);\n    predictions.push(prediction);\n  }\n  cost /= m;\n  return [predictions, cost];\n}\n\nfunction backwardPropagation(\n  predictions: number[],\n  xTrain: number[],\n  yTrain: number[],\n): [number, number] {\n  const m: number = xTrain.length;\n  let dW: number = 0;\n  let dB: number = 0;\n  for (let i = 0; i < m; i += 1) {\n    dW += (yTrain[i] - predictions[i]) * xTrain[i];\n    dB += yTrain[i] - predictions[i];\n  }\n  dW /= m;\n  dB /= m;\n  return [dW, dB];\n}\n\nfunction trainModel({\n  model,\n  epochs,\n  alpha,\n  xTrain,\n  yTrain,\n}: {\n  model: NanoNeuron;\n  epochs: number;\n  alpha: number;\n  xTrain: number[];\n  yTrain: number[];\n}): number[] {\n  const costHistory: number[] = [];\n  for (let epoch = 0; epoch < epochs; epoch += 1) {\n    const [predictions, cost]: [number[], number] = forwardPropagation(\n      model,\n      xTrain,\n      yTrain,\n    );\n    costHistory.push(cost);\n\n    const [dW, dB]: [number, number] = backwardPropagation(\n      predictions,\n      xTrain,\n      yTrain,\n    );\n\n    model.w += alpha * dW;\n    model.b += alpha * dB;\n  }\n  return costHistory;\n}\n",x={title:"NanoNeuron",description:"NanoNeuron",hide_table_of_contents:!0},f=void 0,y={},T=[{value:"Definition",id:"definition",level:2},{value:"Practice",id:"practice",level:2}];function g(n){const e={code:"code",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h2,{id:"definition",children:"Definition"}),"\n",(0,r.jsxs)(o.A,{queryString:"primary",children:[(0,r.jsx)(s.A,{value:"definition",label:"Definition",children:(0,r.jsx)(e.p,{children:"NanoNeuron is a simplified neural network algorithm designed to predict numerical values. It consists of a single neuron with one input and one output, incorporating basic principles of linear regression. The neuron has two parameters: weight (w) and bias (b). The algorithm aims to learn these parameters to fit a given dataset using gradient descent optimization"})}),(0,r.jsx)(s.A,{value:"how",label:"Explanation",children:(0,r.jsx)(e.p,{children:"Iteratively adjusting its weight and bias parameters to minimize the difference between predicted and actual values. Initially, random values are assigned to weight and bias. Then, for each data point, the predicted output is calculated by multiplying the input with the weight, adding the bias, and passing the result through an activation function (usually the identity function in this case). The algorithm calculates the loss (difference between predicted and actual output) and adjusts the parameters using gradient descent to minimize this loss. This process continues for a fixed number of iterations or until the loss converges to a satisfactory level"})}),(0,r.jsx)(s.A,{value:"guidance",label:"Guidance",children:(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Initialize weight (",(0,r.jsx)(e.code,{children:"w"}),") and bias (",(0,r.jsx)(e.code,{children:"b"}),") with random values"]}),"\n",(0,r.jsxs)(e.li,{children:["For each data point (",(0,r.jsx)(e.code,{children:"input"}),", ",(0,r.jsx)(e.code,{children:"output pair"}),") in the dataset","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["calculate the predicted output (",(0,r.jsx)(e.code,{children:"y_pred"}),") using the formula: ",(0,r.jsx)(e.code,{children:"y_pred = w * input + b"})]}),"\n",(0,r.jsxs)(e.li,{children:["compute the loss (",(0,r.jsx)(e.code,{children:"error"}),") between the predicted output and actual output"]}),"\n",(0,r.jsxs)(e.li,{children:["update the weight and bias using gradient descent","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"calculate the gradients of the loss function with respect to weight and bias"}),"\n",(0,r.jsx)(e.li,{children:"adjust weight and bias in the opposite direction of the gradients to minimize loss"}),"\n",(0,r.jsxs)(e.li,{children:["update weight: ",(0,r.jsx)(e.code,{children:"w = w - learning_rate * dw"})]}),"\n",(0,r.jsxs)(e.li,{children:["update bias: ",(0,r.jsx)(e.code,{children:"b = b - learning_rate * db"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.li,{children:"repeat steps for a fixed number of iterations or until convergence criteria are met"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.li,{children:"Once training is complete, the neuron's weight and bias parameters represent the learned model"}),"\n"]})}),(0,r.jsx)(s.A,{value:"tips",label:"Tips",children:(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"choose an appropriate learning rate to balance convergence speed and stability"}),"\n",(0,r.jsx)(e.li,{children:"monitor the loss during training to ensure it is decreasing steadily"}),"\n",(0,r.jsx)(e.li,{children:"normalize the input data to a similar scale to aid convergence"}),"\n",(0,r.jsx)(e.li,{children:"experiment with different activation functions and initialization methods for the neuron's parameters"}),"\n",(0,r.jsx)(e.li,{children:"regularize the model if overfitting occurs by adding regularization terms to the loss function"}),"\n"]})})]}),"\n",(0,r.jsx)(e.h2,{id:"practice",children:"Practice"}),"\n",(0,r.jsxs)(o.A,{queryString:"primary",children:[(0,r.jsx)(s.A,{value:"practice",label:"Practice",children:(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"# Initialize parameters\nw = random()\nb = random()\nlearning_rate = 0.01\nnum_iterations = 1000\n\n# Training loop\nfor _ in range(num_iterations):\n  # Iterate over dataset\n  for input, output in dataset:\n    # Forward pass\n    y_pred = w * input + b\n    # Compute loss\n    loss = (y_pred - output)**2 / 2\n    # Backward pass (gradient descent)\n    dw = (y_pred - output) * input\n    db = y_pred - output\n    # Update parameters\n    w = w - learning_rate * dw\n    b = b - learning_rate * db\n"})})}),(0,r.jsx)(s.A,{value:"solution",label:"Solution",children:(0,r.jsxs)(o.A,{queryString:"code",children:[(0,r.jsx)(s.A,{value:"go",label:"",attributes:{title:"Go Lang",className:"code_lang go m"},children:(0,r.jsx)(d.A,{language:"go",children:l})}),(0,r.jsx)(s.A,{value:"java",label:"",attributes:{title:"Java",className:"code_lang java m"},children:(0,r.jsx)(d.A,{language:"java",children:c})}),(0,r.jsx)(s.A,{value:"js",label:"",attributes:{title:"JavaScript",className:"code_lang js m"},children:(0,r.jsx)(d.A,{language:"js",children:u})}),(0,r.jsx)(s.A,{value:"kotlin",label:"",attributes:{title:"Kotlin",className:"code_lang kotlin m"},children:(0,r.jsx)(d.A,{language:"kotlin",children:p})}),(0,r.jsx)(s.A,{value:"python",label:"",attributes:{title:"Python",className:"code_lang python m"},children:(0,r.jsx)(d.A,{language:"python",children:h})}),(0,r.jsx)(s.A,{value:"rust",label:"",attributes:{title:"Rust",className:"code_lang rust m"},children:(0,r.jsx)(d.A,{language:"rust",children:b})}),(0,r.jsx)(s.A,{value:"ts",label:"",attributes:{title:"TypeScript",className:"code_lang ts m"},children:(0,r.jsx)(d.A,{language:"ts",children:m})})]})})]})]})}function w(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(g,{...n})}):g(n)}}}]);